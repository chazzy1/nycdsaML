library(glmnet)
library(tidyverse)
library(mice)
library(e1071)
library(Metrics)
library(randomForest)
library(glmnet)
install.packages("Metrics")
setwd("~/nyc_bootcamp/03.project/nycdsaML/ref")
# Reading data
train <- read.csv("../data/train.csv", stringsAsFactors = F, sep=',')
test <- read.csv("../data/test.csv", stringsAsFactors = F, sep=',')
# Combining test and train data
full <- bind_rows(train,test)
SalePrice <- train$SalePrice
N <- length(SalePrice)
Id <- test$Id
full[,c('Id','SalePrice')] <- NULL
rm(train,test)
# Converting predictors to factor or integer
chr <- full[,sapply(full,is.character)]
int <- full[,sapply(full,is.integer)]
fac <- chr %>% lapply(as.factor) %>% as.data.frame()
full <- bind_cols(fac,int)
# Running MICE based on random forest
micemod <- full %>% mice(method='rf')
full <- complete(micemod)
# Saving train and test sets
train <- full[1:N,]
test<-full[(N+1):nrow(full),]
# Adding dependent variable
train <- cbind(train,SalePrice)
# Modelling: SVM
svm_model <- svm(SalePrice~., data=train, cost = 3.2)
svm_pred_train <- predict(svm_model,newdata = train)
sqrt(mean((log(svm_pred_train)-log(train$SalePrice))^2))
svm_pred <- predict(svm_model,newdata = test)
write.csv(solution,"svm_solution_32.csv",row.names = F)
# Writing final predictions to CSV file
solution <- data.frame(Id=Id,SalePrice=svm_pred)
write.csv(solution,"svm_solution_32.csv",row.names = F)
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
View(prostate)
set.seed(0)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
y.test = y[test]
train = prostate[test]
length(train)/nrow(x)
# 2
train = sample(1:nrow(x), 7*nrow(x)/10)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
set.seed(0)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
# 2
train = sample(1:nrow(prostate), 7*nrow(prostate)/10)
test = (-train)
y.test = y[test]
y.test = prostate[test]
# 2
smp_size <- floor(0.8 * nrow(prostate))
# Q1
# 1
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
set.seed(0)
# 2
smp_size <- floor(0.8 * nrow(prostate))
# 2
smp_size <- floor(0.8 * nrow(prostate))
set.seed(0)
train_ind <- sample(seq_len(nrow(prostate)), size = smp_size)
train <- prostate[train_ind, ]
test <- prostate[-train_ind, ]
# 3
library(glmnet)
lambda = 10^seq(5, -2, length = 100)
ridge.models = glmnet(train, test, alpha = 0, lambda = grid)
View(test)
y = train[,-1]
View(y)
y = train$lpsa
y = train$lpsa
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
grid = 10^seq(5, -2, length = 100)
y = train$lpsa
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
grid
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
##########################
#####Ridge Regression#####
##########################
library(ISLR)
names(Hitters)
head(Hitters)
#Need matrices for glmnet() function. Automatically conducts conversions as well
#for factor variables into dummy variables.
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
head(x)
x
View(x)
x = train[,-1]
y = train$lpsa
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
head(Hitters)
x = model.matrix(Salary ~ ., train)[, -1] #Dropping the intercept column.
x = model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
y = train$lpsa
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
# Q1
# 1
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
# 2
smp_size <- floor(0.8 * nrow(prostate))
set.seed(0)
train_ind <- sample(seq_len(nrow(prostate)), size = smp_size)
train <- prostate[train_ind, ]
test <- prostate[-train_ind, ]
# 3
library(glmnet)
grid = 10^seq(5, -2, length = 100)
grid
x = model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
View(x)
y = train$lpsa
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
x = cv.glmnet(lpsa ~ ., data=train[-1], family="multinomial")
x = model.matrix(lpsa ~ ., data=train[-1], family="multinomial")
y = train$lpsa
ridge.models = glmnet(train, y, alpha = 0, lambda = grid)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
# 3
library(glmnet)
grid = 10^seq(5, -2, length = 100)
x = model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
y = train$lpsa
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
summary(ridge.models)
dim(coef(ridge.models))
#Need matrices for glmnet() function. Automatically conducts conversions as well
#for factor variables into dummy variables.
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
head(x)
head(Hitters)
help('model')
#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100)
grid
#Fitting the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models) #Inspecting the various coefficient estimates.
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
# 2
smp_size <- floor(0.8 * nrow(prostate))
set.seed(0)
train_ind <- sample(seq_len(nrow(prostate)), size = smp_size)
train <- prostate[train_ind, ]
test <- prostate[-train_ind, ]
# 3
library(glmnet)
grid = 10^seq(5, -2, length = 100)
x = model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
y = train$lpsa
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models))
coef(ridge.models)
ridge.models.coef <- coef(ridge.models)
# 4
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# 5
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
cv.ridge.out = cv.glmnet(x, y,
lambda = grid, alpha = 0, nfolds = 10)
cv.ridge.out
# 6
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
# 7
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = x)
y.test
# 7
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = x)
mean((ridge.bestlambdatrain - y)^2)
# 3
library(glmnet)
grid <- 10^seq(5, -2, length = 100)
train.x <- model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
train.y <- train$lpsa
ridge.models = glmnet(train.x, train.y, alpha = 0, lambda = grid)
dim(coef(ridge.models))
ridge.models.coef <- coef(ridge.models)
# 4
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# 5
set.seed(0)
cv.ridge.out = cv.glmnet(x, y,
lambda = grid, alpha = 0, nfolds = 10)
cv.ridge.out
# 6
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge # 0.1353048
# 7
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = x)
mean((ridge.bestlambdatrain - y)^2) # 0.4538986
0.4538986
# 7
test.x <- model.matrix(lpsa ~ ., test)[, -1] #Dropping the intercept column.
test.y <- test$lpsa
# 7
test.x <- model.matrix(lpsa ~ ., test)[, -1] #Dropping the intercept column.
test.y <- test$lpsa
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
mean((ridge.bestlambdatrain - test.y)^2) # 0.4538986
ridge.models.coef
cv.ridge.out
#######################################################
#######################################################
###### [06] Regularization and Cross validation #######
#######################################################
#######################################################
##########################
#####Ridge Regression#####
##########################
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
names(Hitters)
head(Hitters)
#Need matrices for glmnet() function. Automatically conducts conversions as well
#for factor variables into dummy variables.
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
head(x)
head(Hitters)
help('model')
#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100)
grid
#Fitting the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models) #Inspecting the various coefficient estimates.
#What do the estimates look like for a smaller value of lambda?
ridge.models$lambda[80] #Lambda = 0.2595.
coef(ridge.models)[, 80] #Estimates not close to 0.
sqrt(sum(coef(ridge.models)[-1, 80]^2)) #L2 norm is 136.8179.
#What do the estimates look like for a larger value of lambda?
ridge.models$lambda[15] #Lambda = 10,235.31.
coef(ridge.models)[, 15] #Most estimates close to 0.
sqrt(sum(coef(ridge.models)[-1, 15]^2)) #L2 norm is 7.07.
#Visualizing the ridge regression shrinkage.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#Can use the predict() function to obtain ridge regression coefficients for a
#new value of lambda, not necessarily one that was within our grid:
predict(ridge.models, s = 50, type = "coefficients")
#Creating training and testing sets. Here we decide to use a 70-30 split with
#approximately 70% of our data in the training set and 30% of our data in the
#test set.
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
#Let's attempt to fit a ridge regression using some arbitrary value of lambda;
#we still have not yet figured out what the best value of lambda should be!
#We will arbitrarily choose 5. We will now use the training set exclusively.
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2) # MSE
# Q1
# 1
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
# 2
smp_size <- floor(0.8 * nrow(prostate))
set.seed(0)
train_ind <- sample(seq_len(nrow(prostate)), size = smp_size)
train <- prostate[train_ind, ]
test <- prostate[-train_ind, ]
# 3
library(glmnet)
grid <- 10^seq(5, -2, length = 100)
train.x <- model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
train.y <- train$lpsa
ridge.models = glmnet(train.x, train.y, alpha = 0, lambda = grid)
dim(coef(ridge.models))
ridge.models.coef <- coef(ridge.models)
# 4
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# 5
set.seed(0)
cv.ridge.out = cv.glmnet(x, y,
lambda = grid, alpha = 0, nfolds = 10)
cv.ridge.out
# 6
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge # 0.1353048
# 7
test.x <- model.matrix(lpsa ~ ., test)[, -1] #Dropping the intercept column.
test.y <- test$lpsa
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
ridge.lambda = predict(ridge.bestlambdatrain, s = 5, newx = test.x)
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
ridge.lambda = predict(ridge.bestlambdatrain, s = 5, newx = test.x)
mean((ridge.lambda - test.y)^2) # 0.4913108
test.x <- model.matrix(lpsa ~ ., test)[, -1] #Dropping the intercept column.
test.y <- test$lpsa
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
ridge.lambda = predict(ridge.bestlambdatrain, s = 5, newx = test.x)
mean((ridge.lambda - test.y)^2) # 0.4913108
ridge.lambda = predict(ridge.bestlambdatrain, s = 5, newx = test.x)
ridge.models.train = glmnet(test.x, test.y, alpha = 0, lambda = bestlambda.ridge)
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
ridge.models.train = glmnet(test.x, test.y, alpha = 0, lambda = bestlambda.ridge)
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
mean((ridge.bestlambdatrain - test.y)^2) # 0.4913108
#######################################################
#######################################################
###### [06] Regularization and Cross validation #######
#######################################################
#######################################################
##########################
#####Ridge Regression#####
##########################
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
names(Hitters)
head(Hitters)
#Need matrices for glmnet() function. Automatically conducts conversions as well
#for factor variables into dummy variables.
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
head(x)
head(Hitters)
help('model')
#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100)
grid
#Fitting the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models) #Inspecting the various coefficient estimates.
#What do the estimates look like for a smaller value of lambda?
ridge.models$lambda[80] #Lambda = 0.2595.
coef(ridge.models)[, 80] #Estimates not close to 0.
sqrt(sum(coef(ridge.models)[-1, 80]^2)) #L2 norm is 136.8179.
#What do the estimates look like for a larger value of lambda?
ridge.models$lambda[15] #Lambda = 10,235.31.
coef(ridge.models)[, 15] #Most estimates close to 0.
sqrt(sum(coef(ridge.models)[-1, 15]^2)) #L2 norm is 7.07.
#Visualizing the ridge regression shrinkage.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#Can use the predict() function to obtain ridge regression coefficients for a
#new value of lambda, not necessarily one that was within our grid:
predict(ridge.models, s = 50, type = "coefficients")
#Creating training and testing sets. Here we decide to use a 70-30 split with
#approximately 70% of our data in the training set and 30% of our data in the
#test set.
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
#Let's attempt to fit a ridge regression using some arbitrary value of lambda;
#we still have not yet figured out what the best value of lambda should be!
#We will arbitrarily choose 5. We will now use the training set exclusively.
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2) # MSE
#Here, the MSE is approximately 115,541.
#What would happen if we fit a ridge regression with an extremely large value
#of lambda? Essentially, fitting a model with only an intercept:
ridge.largelambda = predict(ridge.models.train, s = 1e10, newx = x[test, ])
mean((ridge.largelambda - y.test)^2)
#Here, the MSE is much worse at aproximately 208,920.
#Instead of arbitrarily choosing random lambda values and calculating the MSE
#manually, it's a better idea to perform cross-validation in order to choose
#the best lambda over a slew of values.
#Running 10-fold cross validation.
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
cv.ridge.out
#What is the test MSE associated with this best value of lambda?
ridge.bestlambdatrain = predict(ridge.models.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
#Here the MSE is lower at approximately 113,173; a further improvement
#on that which we have seen above. With "cv.ridge.out", we can actually access
#the best model from the cross validation without calling "ridge.models.train"
#or "bestlambda.ridge":
ridge.bestlambdatrain = predict.cv.glmnet(cv.ridge.out, s ="lambda.min", newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
cv.ridge.out
bestlambda.ridge
ridge.bestlambdatrain = predict(ridge.models.train, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
# Q1
# 1
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
# 2
smp_size <- floor(0.8 * nrow(prostate))
set.seed(0)
train_ind <- sample(seq_len(nrow(prostate)), size = smp_size)
train <- prostate[train_ind, ]
test <- prostate[-train_ind, ]
# 3
library(glmnet)
grid <- 10^seq(5, -2, length = 100)
train.x <- model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
train.y <- train$lpsa
ridge.models = glmnet(train.x, train.y, alpha = 0, lambda = grid)
dim(coef(ridge.models))
ridge.models.coef <- coef(ridge.models)
# 4
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# 5
set.seed(0)
cv.ridge.out = cv.glmnet(x, y,
lambda = grid, alpha = 0, nfolds = 10)
cv.ridge.out
# 6
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge # 0.1353048
# 7
test.x <- model.matrix(lpsa ~ ., test)[, -1] #Dropping the intercept column.
test.y <- test$lpsa
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
mean((ridge.bestlambdatrain - y.test)^2)
mean((ridge.bestlambdatrain - test.y)^2)
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
mean((ridge.bestlambdatrain - test.y)^2)
ridge.bestlambdatrain
test.y
mean((ridge.bestlambdatrain - test.y)^2)
#
ridge.bestlambdatrain = predict.cv.glmnet(cv.ridge.out, s ="lambda.min", newx =test.x)
# Q1
# 1
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
# 2
smp_size <- floor(0.8 * nrow(prostate))
set.seed(0)
train_ind <- sample(seq_len(nrow(prostate)), size = smp_size)
train <- prostate[train_ind, ]
test <- prostate[-train_ind, ]
# 3
library(glmnet)
grid <- 10^seq(5, -2, length = 100)
train.x <- model.matrix(lpsa ~ ., train)[, -1] #Dropping the intercept column.
train.y <- train$lpsa
ridge.models = glmnet(train.x, train.y, alpha = 0, lambda = grid)
dim(coef(ridge.models))
ridge.models.coef <- coef(ridge.models)
# 4
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
# 5
set.seed(0)
cv.ridge.out = cv.glmnet(x, y,
lambda = grid, alpha = 0, nfolds = 10)
cv.ridge.out
# 6
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge # 0.1353048
# 7
test.x <- model.matrix(lpsa ~ ., test)[, -1] #Dropping the intercept column.
test.y <- test$lpsa
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = test.x)
mean((ridge.bestlambdatrain - test.y)^2) # 0.712
#
ridge.bestlambdatrain = predict.cv.glmnet(cv.ridge.out, s ="lambda.min", newx =test.x)
mean((ridge.bestlambdatrain - y.test)^2)
